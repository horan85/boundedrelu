# -*- coding: utf-8 -*-
"""BoundedReLUTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AhzLh4XPlaYRjtOKcGrdnnWEjZV5F7T4
"""

import numpy as np
import random
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

#read data and display data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) # we will use one hot encoding, every outputclass is a separate dimension

print(mnist.train.images.shape)
print(mnist.train.labels.shape) # we have train images and train labels
print(mnist.test.images.shape)
print(mnist.test.labels.shape) # ans separate test iamges and labels for evaluation
#print(mnist.train.images[0,:])
mnist.train.labels[0]

#display an image just to get a feel what we are working on
import matplotlib.pyplot as plt
img= np.reshape(mnist.train.images[1,:],[28,28])

#notebook
fig,ax =plt.subplots()
ax.imshow(img,cmap="Greys")
display(fig)

#python script
plt.imshow(img,cmap="Greys")
plt.show()

#the parameters of the algorithm
BatchLength= 128  #batches of 32 images are processed and averaged out
Size=[28,28,1]  #size of the input image
NumIteration=201 #we will run just a few itereations- you can run it longer at home
LearningRate=1e-4 #initial learning rate
NumClasses = 10 #number of possible output classes
EvalFreq = 10 # we will evaluate at every 1000 step

tf.reset_default_graph() #reset the graph
InputData = tf.placeholder(tf.float32, [None]+Size )  #input images
OneHotLabels = tf.placeholder(tf.int32, [None, NumClasses]) #the expected outputs, labels
print(InputData)
NumKernels=[32,32,32] #a list that defines the number of layers and how many convolution kernels we have at each layer

def AddBoundedLeakyRelu(x):
    alpha=0.01
    R=tf.maximum(-1 + alpha*(x+1),x)
    R=tf.minimum(1 + alpha*(R-1),R)
    return R

def BaseLineNetwork(Input):
  CurrentFilters=Size[2]
  LayerNum=0
  CurrentInput=Input
  # a loop which creates all layers
  for N in NumKernels:
      with tf.variable_scope('baselineconv'+str(LayerNum)):
        LayerNum+=1
        #variables that we want to optimize
        W =tf.get_variable('W', [3,3,CurrentFilters,N])
        Bias = tf.get_variable('Bias', [N],initializer=tf.constant_initializer(0.0))
        #convolution
        ConvResult = tf.nn.conv2d(CurrentInput,W,strides=[1,1,1,1], padding='VALID')
        CurrentFilters=N
        #we adda bias
        ConvResult = tf.add(ConvResult,Bias)
        print(ConvResult)
        # relu
        ReLU=tf.nn.relu(ConvResult)
        #pool
        Pooled=tf.nn.max_pool(ReLU,ksize=[1,3,3,1],strides=[1,1,1,1],padding='VALID')
        CurrentInput=Pooled
        print(Pooled)
  #we have generated feature maps, we will use a fully connected layer with ten neurons, one for each class
  #the response of these neruons will represent how "strongly" the element belong to this class
  with tf.variable_scope('BaselineFC'):
        CurrentShape=CurrentInput.get_shape()
        FeatureLength = int(CurrentShape[1]*CurrentShape[2]*CurrentShape[3])
        FC = tf.reshape(CurrentInput, [-1, FeatureLength])
        W = tf.get_variable('W',[FeatureLength,NumClasses])
        FC = tf.matmul(FC, W)
        Bias = tf.get_variable('Bias',[NumClasses])
        FC = tf.add(FC, Bias)
  return FC

def BoundedNetwork(Input):
  CurrentFilters=Size[2]
  LayerNum=0
  CurrentInput=Input
  # a loop which creates all layers
  for N in NumKernels:
      with tf.variable_scope('boundedconv'+str(LayerNum)):
        LayerNum+=1
        #variables that we want to optimize
        W =tf.get_variable('W', [3,3,CurrentFilters,N])
        Bias = tf.get_variable('Bias', [N],initializer=tf.constant_initializer(0.0))
        #convolution
        ConvResult = tf.nn.conv2d(CurrentInput,W,strides=[1,1,1,1], padding='VALID')
        CurrentFilters=N
        #we adda bias
        ConvResult = tf.add(ConvResult,Bias)
        print(ConvResult)
        # relu
        #ReLU=tf.nn.relu(ConvResult)
        ReLU=AddBoundedLeakyRelu(ConvResult)
        #pool
        Pooled=tf.nn.max_pool(ReLU,ksize=[1,3,3,1],strides=[1,1,1,1],padding='VALID')
        CurrentInput=Pooled
        print(Pooled)
  #we have generated feature maps, we will use a fully connected layer with ten neurons, one for each class
  #the response of these neruons will represent how "strongly" the element belong to this class
  with tf.variable_scope('boundedFC'):
        CurrentShape=CurrentInput.get_shape()
        FeatureLength = int(CurrentShape[1]*CurrentShape[2]*CurrentShape[3])
        FC = tf.reshape(CurrentInput, [-1, FeatureLength])
        W = tf.get_variable('W',[FeatureLength,NumClasses])
        FC = tf.matmul(FC, W)
        Bias = tf.get_variable('Bias',[NumClasses])
        FC = tf.add(FC, Bias)
  return FC

BoundedOut=BoundedNetwork(InputData)
BaseLineOut=BaseLineNetwork(InputData)

#we use softmax to normalize the outputs of the network 
#sotfmax camoes from the logistic regression and is e^i / sum(e^j) for all j
#this will normlaize all the values between zero and one and the sum of values will be 1

#corss entropy measures similarity between two distributions. If cross entropy is zero, the two distributions are the same
with tf.name_scope('loss'):
	    BaselineLoss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=OneHotLabels,logits=BaseLineOut)  )
	    BoundedLoss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=OneHotLabels,logits=BoundedOut)  )
      
with tf.name_scope('optimizer'):    
	    #Use ADAM optimizer this is currently the best performing training algorithm in most cases
	    BaselineOptimizer = tf.train.AdamOptimizer(LearningRate).minimize(BaselineLoss)
	    BoundedOptimizer = tf.train.AdamOptimizer(LearningRate).minimize(BoundedLoss)
        #Optimizer = tf.train.GradientDescentOptimizer(LearningRate).minimize(Loss)

with tf.name_scope('accuracy'):	  
	    CorrectPredictions = tf.equal(tf.argmax(BaseLineOut, 1), tf.argmax(OneHotLabels, 1))
	    BaselineAccuracy = tf.reduce_mean(tf.cast(CorrectPredictions, tf.float32))
	    CorrectPredictions = tf.equal(tf.argmax(BoundedOut, 1), tf.argmax(OneHotLabels, 1))
	    BoundedAccuracy = tf.reduce_mean(tf.cast(CorrectPredictions, tf.float32))

Init = tf.global_variables_initializer()
BaseTrain=[]
BoundedTrain=[]
BaseLoss=[]
BoundedLloss=[]
BaseTest=[]
BoundedTest=[]
with tf.Session() as Sess:
	Sess.run(Init)
	
	Step = 1
	# Keep training until reach max iterations - other stopping criterion could be added
	while Step < NumIteration:
		UsedInBatch= random.sample( range(mnist.train.images.shape[0]), BatchLength)
		batch_xs = mnist.train.images[UsedInBatch,:]
		batch_ys = mnist.train.labels[UsedInBatch,:]
		batch_xs=np.reshape(batch_xs,[BatchLength]+Size)
		_,BaseAcc,BaseL = Sess.run([BaselineOptimizer, BaselineAccuracy, BaselineLoss], feed_dict={InputData: batch_xs, OneHotLabels: batch_ys})
		_,BoundedAcc,BoundedL = Sess.run([BoundedOptimizer, BoundedAccuracy,BoundedLoss], feed_dict={InputData: batch_xs, OneHotLabels: batch_ys})
		
		if (Step%10)==0:
	        	print("Iteration: "+str(Step) + " BaseAccuracy: " + str(BaseAcc) + " BaseLoss: " + str(BaseL))
	        	print("Iteration: "+str(Step) + " BoundedAccuracy: " + str(BoundedAcc) + " BoundedLoss: " + str(BoundedL))
	        	BaseTrain.append(BaseAcc)
	        	BoundedTrain.append(BoundedAcc)
	        	BaseLoss.append(BaseL)
	        	BoundedLloss.append(BoundedL)
		
		#independent test accuracy
    		if (Step%EvalFreq)==0:
      			SumAcc=0.0
      			for i in range(0,mnist.test.images.shape[0]):
        			batch_xs = mnist.test.images[i,:]
        			batch_ys = mnist.test.labels[i,:]
        			batch_xs=np.reshape(batch_xs,[1]+Size)
        			batch_ys=np.reshape(batch_ys,[1,NumClasses])
        			a = Sess.run(BaselineAccuracy, feed_dict={InputData: batch_xs, OneHotLabels: batch_ys})
        			SumAcc+=a
      			print("Independent Baseline Test set: "+str(float(SumAcc)/mnist.test.images.shape[0]))
       			BaseTest.append( float(SumAcc)/mnist.test.images.shape[0])
      			SumAcc=0.0
      			for i in range(0,mnist.test.images.shape[0]):
        			batch_xs = mnist.test.images[i,:]
        			batch_ys = mnist.test.labels[i,:]
        			batch_xs=np.reshape(batch_xs,[1]+Size)
        			batch_ys=np.reshape(batch_ys,[1,NumClasses])
        			a = Sess.run(BoundedAccuracy, feed_dict={InputData: batch_xs, OneHotLabels: batch_ys})
        			SumAcc+=a  
      			print("Independent Bounded Test set: "+str(float(SumAcc)/mnist.test.images.shape[0]))
      			BoundedTest.append(float(SumAcc)/mnist.test.images.shape[0])
    		Step+=1

import matplotlib.pyplot as plt
Length=range(len(BaseTrain))
plt.plot(Length,BaseTrain,'b',Length,BoundedTrain,'r')

Length=range(len(BaseTest))
plt.plot(Length,BaseTest,'b',Length,BoundedTest,'r')

Length=range(len(BaseLoss))
plt.plot(Length,BaseLoss,'b',Length,BoundedLloss,'r')

